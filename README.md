### SMALLER MODELS CAN PACK A PUNCH IN THE ERA OF LARGE LANGUAGE MODELS
As the pursuit of ever-larger AI models continues, an important question arises - is massive scale the only path forward? Our talk presents a family of models in the 7 to 13 billion parameter range that demonstrate smaller can be mighty if engineered thoughtfully. With innovations in attention and efficiency, these nimble models match or even exceed the performance of prior work with significantly larger parameter counts. Specifically we look at models like Mistral 7B, a recently released model with innovations like grouped-query and sliding window attention. Mistral 7B is more efficient and effective than prior models in the same size regime, but it also beats the previous best 13 billion parameter model on all tests, even matching some 34 billion models in reasoning and math. These efficient designs represent a promising path to optimize large language models for real-world usage. Our talk shares insights from this work that can guide the community to build models balancing performance, efficiency, and scalability. This opens the door to an era of precise and powerful AI that doesn't require a growing number of resources.

https://neurips.cc/Expo/Conferences/2023/talk%20panel/78244

## NeurIPS Large Language Model Efficiency Challenge: 1 LLM + 1GPU + 1Day
Large Language Models (LLMs) have been pivotal in the recent Cambrian explosion of generative AI applications. However, existing efforts to democratize access to fine-tune and query LLMs have been largely limited by growing hardware costs required to adapt and serve these models. Enabling low cost and efficient LLM fine-tuning and inference can have significant impact on industrial and scientific applications. Here, we present a single GPU fine-tuning and inference competition. Our goal is to accelerate the development of practical software methods to reduce the costs associated with utilizing LLMs. Furthermore, by advocating for goal-oriented and infrastructure-focused evaluation frameworks that stress reproducibility, our aim is to democratize access to these methods and enhance their accessibility to the wider public.

https://neurips.cc/virtual/2023/competition/66594

### Representation Engineering Mistral-7B an Acid Trip

[https://vgel.me/posts/representation-engineering/](https://vgel.me/posts/representation-engineering/)

### Fine Tuning Mistral 7B on Magic the Gathering Drafts

[https://generallyintelligent.substack.com/p/fine-tuning-mistral-7b-on-magic-the](https://generallyintelligent.substack.com/p/fine-tuning-mistral-7b-on-magic-the)

### How we built “Mistral 7B Fine-Tune Optimized,” the best 7B model for fine-tuning

[https://openpipe.ai/blog/mistral-7b-fine-tune-optimized](https://openpipe.ai/blog/mistral-7b-fine-tune-optimized)

### How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024

[https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)

[https://docs.helix.ml/helix/](https://docs.helix.ml/helix/)

### NeuralFlow – Visualize the intermediate output of Mistral 7B

[https://github.com/valine/NeuralFlow](https://github.com/valine/NeuralFlow)

### Mistral source code

https://github.com/mistralai/mistral-src
